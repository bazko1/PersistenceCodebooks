{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation with splits from paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../..\")\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1, 5700)\n"
     ]
    }
   ],
   "source": [
    "# Load diagrams\n",
    "data_path = \"./pds.mat\"\n",
    "data_mat = scipy.io.loadmat(data_path)\n",
    "data = data_mat[\"pds\"]\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "y = data_mat[\"labels\"]\n",
    "y = np.array(y).ravel()\n",
    "\n",
    "X = data.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits, these are indexes of points in X, matlab index from 1 (so we do -1)\n",
    "splits = scipy.io.loadmat('./presplited/3dshapes_splits.mat')\n",
    "test_set = np.array( [arr[0][0] for arr in (splits['testSet'] - 1)] )\n",
    "train_set = np.array( [arr[0][0] for arr in (splits['trainSet'] - 1)] )\n",
    "\n",
    "tt_pairs = np.array([np.array(el) for el in zip(train_set, test_set)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: Model hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "from preprocessing import *\n",
    "from persistent_bow import *\n",
    "from visualization import *\n",
    "import experiments.utils as ut\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from gudhi.representations.kernel_methods import SlicedWassersteinKernel\n",
    "from gudhi.representations.vector_methods import PersistenceImage\n",
    "from gudhi.representations.metrics import BottleneckDistance\n",
    "\n",
    "#Fast hack to make SlicedWassersteinKernel scikit-compliant\n",
    "setattr(SlicedWassersteinKernel, \"get_params\",\n",
    "        lambda self, deep: {\n",
    "            \"bandwidth\":self.bandwidth,\n",
    "            \"num_directions\" : self.sw_.num_directions\n",
    "        })\n",
    "\n",
    "fold = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "splits = np.array([split for split in fold.split(X, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function, constructs final pipeline and returns girdsearch for it\n",
    "def make_final_grid(estimator, param_grid, kernel=\"linear\", *args, **kwargs ):\n",
    "    new_param_grid = {f\"Model__{name}\" : values for name, values in param_grid.items()}\n",
    "    new_param_grid[\"Predictor__C\"] = [0.1, 1, 10]\n",
    "    \n",
    "    final_pipeline = Pipeline([\n",
    "        (\"Model\", estimator),\n",
    "        (\"Predictor\", SVC(kernel=kernel, max_iter=1e4))\n",
    "    ])\n",
    "    \n",
    "    return GridSearchCV(final_pipeline, new_param_grid, cv = tt_pairs\n",
    "                        , *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight functions\n",
    "\n",
    "def const(x):\n",
    "    return 1\n",
    "\n",
    "def linear(x):\n",
    "    return x[1]\n",
    "\n",
    "def pow2(x):\n",
    "    return x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PBoW gridsearch\n",
    "pbow_gridsearch = make_final_grid(\n",
    "    estimator = PersistentBow(KMeans(7, n_init=1, max_iter=100, random_state=42),\n",
    "                              sampler=RandomPDSampler(2500, random_state=42)),\n",
    "    param_grid = {\n",
    "        \"cluster__n_clusters\" : np.arange(10, 200, 10),\n",
    "        \"sampler__max_points\" : np.arange(1000, 13000, 1000),\n",
    "        \"sampler__weight_function\" : [const, linear, pow2]\n",
    "    },\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "#SPBoW gridsearch\n",
    "spbow_gridsearch = make_final_grid(\n",
    "    estimator = StablePersistentBow(GaussianMixture(random_state=42),\n",
    "                              sampler=RandomPDSampler(2500, random_state=42)),\n",
    "    param_grid = {\n",
    "        \"mixture__n_components\" : np.arange(10, 200, 10),\n",
    "        \"sampler__max_points\" : np.arange(1000, 13000, 1000),\n",
    "        \"sampler__weight_function\" : [const, linear, pow2]\n",
    "    },\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "# PBOW with KMeans from VLFeat # TODO: This is Vl kmeans are 2 functions only\n",
    "# We need to wrap it with something what fits fit, transform API\n",
    "# pbow_on_stereoids = pbow_pipeline = Pipeline([\n",
    "#     (\"pbow\",  PersistentBow(Vlkmeans(7,2), sampler=RandomPDSampler(2500)))\n",
    "# ])\n",
    "\n",
    "\n",
    "#SlicedWassersteinKernel gridsearch (without using it as kernel)\n",
    "swk_gridsearch = make_final_grid(\n",
    "    estimator = SlicedWassersteinKernel(),\n",
    "    param_grid = {\n",
    "        \"bandwidth\" : [0.05, 0.1, 0.25, 0.5, 1, 1.5, 2],\n",
    "        \"num_directions\" : [5, 10, 15, 20, 25]\n",
    "    },\n",
    "    n_jobs = -1    \n",
    ")\n",
    "\n",
    "#SlicedWassersteinKernel gridsearch (using it as kernel)\n",
    "swk_ker_gridsearch = make_final_grid(\n",
    "    estimator = SlicedWassersteinKernel(),\n",
    "    param_grid = {\n",
    "        \"bandwidth\" : [0.05, 0.1, 0.25, 0.5, 1, 1.5, 2],\n",
    "        \"num_directions\" : [5, 10, 15, 20, 25]\n",
    "    },\n",
    "    kernel=\"precomputed\",\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "#BottleneckDistance gridsearch\n",
    "bd_gridsearch = make_final_grid(\n",
    "    estimator = BottleneckDistance(),\n",
    "    param_grid = {\n",
    "        \"epsilon\" : [1e-8, None]\n",
    "    },\n",
    "    n_jobs = -1  \n",
    ")\n",
    "\n",
    "#PersistenceImage gridsearch\n",
    "pi_gridsearch = make_final_grid(\n",
    "    estimator = PersistenceImage(),\n",
    "    param_grid = {\n",
    "        \"bandwidth\" : [0.1, 0.25, 0.5, 1, 1.5],\n",
    "        \"weight\" : [const, linear, pow2],\n",
    "        \"resolution\" : [(10,10), (20,20), (40,40), (50, 50)],\n",
    "    },\n",
    "    n_jobs = -1\n",
    ")\n",
    "\n",
    "models_to_test = {\n",
    "    \"PBoW\" : pbow_gridsearch, \n",
    "    \"SPboW\" : spbow_gridsearch,\n",
    "    \"SWK\" : swk_gridsearch, \n",
    "    \"SWK_ker\" : swk_ker_gridsearch,\n",
    "    \"Bottleneck\" : bd_gridsearch,\n",
    "    \"PersistenceImage\" : pi_gridsearch\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PBoW\n",
      "Loaded from file\n",
      "SPboW\n",
      "Loaded from file\n",
      "SWK\n",
      "Loaded from file\n",
      "SWK_ker\n",
      "Loaded from file\n",
      "Bottleneck\n",
      "Loaded from file\n",
      "PersistenceImage\n",
      "Loaded from file\n"
     ]
    }
   ],
   "source": [
    "for name, grid in models_to_test.items():\n",
    "    print(name, end = \" \")\n",
    "    grid_path = f\"precomputed/grid/{name}.dill\"\n",
    "    \n",
    "    out = load(grid_path)\n",
    "    if out:\n",
    "        print(\"Loaded from file\")\n",
    "        models_to_test[name] = out\n",
    "    else:\n",
    "        grid.verbose = 10\n",
    "        grid.fit(X, y)\n",
    "        save(grid, grid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded results: SWK_ker\n",
      "Loaded results: SPboW\n",
      "Loaded results: PersistenceImage\n",
      "Loaded results: PBoW\n",
      "Computing Bottleneck\n",
      "Skip Bottleneck because it hangs...\n",
      "Loaded results: SWK\n"
     ]
    }
   ],
   "source": [
    "#Grid computation\n",
    "grid_path = f\"./presplited/grid/\"\n",
    "cv_path = f\"./presplited/cv/\"\n",
    "\n",
    "for filename in os.listdir(grid_path):\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    grid = ut.load(os.path.join(grid_path, filename))\n",
    "    results = ut.load(os.path.join(cv_path, filename))\n",
    "    \n",
    "    if not results:\n",
    "        print(\"Computing\", name)\n",
    "        model = grid.best_estimator_\n",
    "        results = cross_validate(model, X, y, cv=tt_pairs, n_jobs=-1, verbose=10)\n",
    "        ut.save(results, os.path.join(cv_path, f\"{name}.dill\"))\n",
    "    else:\n",
    "        print(f\"Loaded results: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PersistenceImage Mean scores:\n",
      "fit_time      28.096989\n",
      "score_time     8.174758\n",
      "test_score     0.894489\n",
      "dtype: float64 \n",
      "\n",
      "SWK_ker Mean scores:\n",
      "fit_time      280.212759\n",
      "score_time    555.229747\n",
      "test_score      0.951000\n",
      "dtype: float64 \n",
      "\n",
      "SPboW Mean scores:\n",
      "fit_time      10.721233\n",
      "score_time     3.950668\n",
      "test_score     0.742857\n",
      "dtype: float64 \n",
      "\n",
      "PBoW Mean scores:\n",
      "fit_time      10.206509\n",
      "score_time     3.118982\n",
      "test_score     0.853914\n",
      "dtype: float64 \n",
      "\n",
      "SWK Mean scores:\n",
      "fit_time      277.187684\n",
      "score_time    541.187843\n",
      "test_score      0.948543\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Printing scores\n",
    "base_path = f\"./presplited/cv/\"\n",
    "for filename in os.listdir(base_path):\n",
    "    path = os.path.join(base_path, filename)\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    results = ut.load(path)\n",
    "    print(name, \"Mean scores:\")\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.mean(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for difference between expperiment and paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PBoW gridsearch with different gamma, random state and params\n",
    "pbow_gridsearch2 = GridSearchCV(\n",
    "    Pipeline([\n",
    "        (\"Model\", PersistentBow(KMeans(7, n_init=1, max_iter=300),\n",
    "                              sampler=RandomPDSampler(2500))),\n",
    "        (\"Estimator\", SVC(kernel=\"linear\", max_iter=1e4, gamma='auto'))\n",
    "    ]),\n",
    "    {\n",
    "        \"Model__cluster__n_clusters\" : np.arange(100, 250, 25),\n",
    "        \"Model__sampler__max_points\" : np.arange(10000, 20000, 2500),\n",
    "        \"Model__sampler__weight_function\" : [const, linear],\n",
    "        \"Estimator__C\" : [0.1, 1, 10, 100, 1000]\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    cv=tt_pairs\n",
    ")\n",
    "\n",
    "#PBoW gridsearch with different linear svc\n",
    "pbow_gridsearch3 = GridSearchCV(\n",
    "    Pipeline([\n",
    "        (\"Model\", PersistentBow(KMeans(7, n_init=1, max_iter=300),\n",
    "                              sampler=RandomPDSampler(2500))),\n",
    "        (\"Estimator\", LinearSVC(max_iter=1e4, penalty=\"L1\", loss=\"hinge\"))\n",
    "    ]),\n",
    "    {\n",
    "        \"Model__cluster__n_clusters\" : np.arange(60, 200, 25),\n",
    "        \"Model__sampler__max_points\" : np.arange(1000, 20000, 2500),\n",
    "        \"Model__sampler__weight_function\" : [const, linear],\n",
    "        \"Estimator__C\" : [0.1, 1, 10],\n",
    "        \"Estimator__dual\" : [True, False],\n",
    "        \"Estimator__loss\" : [\"hinge\", \"squared_hinge\"],\n",
    "        \"Estimator__penalty\" : [\"l1\", \"l2\"],\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    cv=tt_pairs\n",
    ")\n",
    "\n",
    "#PBoW gridsearch without sampling\n",
    "pbow_gridsearch4 = GridSearchCV(\n",
    "    Pipeline([\n",
    "        (\"Model\", PersistentBow(KMeans(7, n_init=10, max_iter=300),\n",
    "                              sampler=RandomPDSampler(1000))),\n",
    "            (\"Estimator\", SVC(kernel=\"linear\", max_iter=1e4, gamma='auto'))\n",
    "    ]),\n",
    "    {\n",
    "        \"Model__cluster__n_clusters\" : np.arange(100, 250, 25),\n",
    "        \"Model__sampler__max_points\" : np.arange(10000, 20000, 2000),\n",
    "        \"Estimator__C\" : [0.1, 1, 2, 5, 7, 10, 25]\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    cv=tt_pairs\n",
    ")\n",
    "\n",
    "#PBoW with different kernels\n",
    "pbow_gridsearch5 = GridSearchCV(\n",
    "    Pipeline([\n",
    "        (\"Model\", PersistentBow(KMeans(7, n_init=1, max_iter=300),\n",
    "                              sampler=RandomPDSampler(2500))),\n",
    "        (\"Estimator\", SVC(kernel=\"linear\", max_iter=1e4, gamma='auto'))\n",
    "    ]),\n",
    "    {\n",
    "        \"Model__cluster__n_clusters\" : np.arange(100, 250, 25),\n",
    "        \"Model__sampler__max_points\" : np.arange(10000, 20000, 2000),\n",
    "        \"Model__sampler__weight_function\" : [const, linear],\n",
    "        \"Estimator__C\" : [0.1, 1, 10, 100, 1000],\n",
    "        \"Estimator__kernel\" : [\"rbf\", \"linear\", \"sigmoid\", \"poly\"]\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    cv=tt_pairs\n",
    ")\n",
    "\n",
    "#PBoW with different kernels on python-version splits\n",
    "pbow_gridsearch6 = GridSearchCV(\n",
    "    Pipeline([\n",
    "        (\"Model\", PersistentBow(KMeans(7, n_init=1, max_iter=300),\n",
    "                              sampler=RandomPDSampler(2500))),\n",
    "        (\"Estimator\", SVC(kernel=\"linear\", max_iter=1e4, gamma='auto'))\n",
    "    ]),\n",
    "    {\n",
    "        \"Model__cluster__n_clusters\" : np.arange(100, 250, 25),\n",
    "        \"Model__sampler__max_points\" : np.arange(10000, 20000, 2000),\n",
    "        \"Model__sampler__weight_function\" : [const, linear],\n",
    "        \"Estimator__C\" : [0.1, 1, 10],\n",
    "        \"Estimator__kernel\" : [\"rbf\", \"linear\", \"sigmoid\", \"poly\"]\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    cv=splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbow2_path = f\"./presplited/secondary/pbow2.dill\" \n",
    "grid = ut.load(pbow2_path)\n",
    "if not grid:\n",
    "    pbow_gridsearch2.verbose = 10\n",
    "    pbow_gridsearch2.fit(X, y)\n",
    "    ut.save(pbow_gridsearch2, pbow2_path)\n",
    "else:\n",
    "    pbow_gridsearch2 = grid\n",
    "\n",
    "pbow3_path = f\"./presplited/secondary/pbow3.dill\" \n",
    "grid = ut.load(pbow3_path)\n",
    "if not grid:\n",
    "    pbow_gridsearch3.verbose = 10\n",
    "    pbow_gridsearch3.fit(X, y)\n",
    "    ut.save(pbow_gridsearch3, pbow3_path)\n",
    "else:\n",
    "    pbow_gridsearch3 = grid\n",
    "    \n",
    "pbow4_path = f\"./presplited/secondary/pbow4.dill\" \n",
    "grid = ut.load(pbow4_path)\n",
    "if not grid:\n",
    "    pbow_gridsearch4.verbose = 10\n",
    "    pbow_gridsearch4.fit(X, y)\n",
    "    ut.save(pbow_gridsearch4, pbow4_path)\n",
    "else:\n",
    "    pbow_gridsearch4 = grid\n",
    "    \n",
    "    \n",
    "pbow5_path = f\"./presplited/secondary/pbow5.dill\" \n",
    "grid = ut.load(pbow5_path)\n",
    "if not grid:\n",
    "    pbow_gridsearch5.verbose = 10\n",
    "    pbow_gridsearch5.fit(X, y)\n",
    "    ut.save(pbow_gridsearch5, pbow5_path)\n",
    "else:\n",
    "    pbow_gridsearch5 = grid\n",
    "    \n",
    "pbow6_path = f\"./presplited/secondary/pbow6.dill\" \n",
    "grid = ut.load(pbow6_path)\n",
    "if not grid:\n",
    "    pbow_gridsearch6.verbose = 10\n",
    "    pbow_gridsearch6.fit(X, y)\n",
    "    ut.save(pbow_gridsearch6, pbow6_path)\n",
    "else:\n",
    "    pbow_gridsearch6 = grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8588276588276589"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbow_gridsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501228501228502"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbow_gridsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8548964548964548"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbow_gridsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8916812916812917"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbow_gridsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8978947368421053"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbow_gridsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Estimator__C': 1,\n",
       " 'Model__cluster__n_clusters': 225,\n",
       " 'Model__sampler__max_points': 12500,\n",
       " 'Model__sampler__weight_function': <function __main__.linear(x)>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbow_gridsearch2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 120 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  50 | elapsed:  2.0min remaining: 31.0min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  50 | elapsed:  2.0min remaining:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  50 | elapsed:  2.2min remaining:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  50 | elapsed:  2.4min remaining:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  50 | elapsed:  2.5min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  50 | elapsed:  2.9min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  50 | elapsed:  3.0min remaining:   50.6s\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  50 | elapsed:  3.0min remaining:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  3.0min finished\n",
      "/home/drenda/miniconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:231: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "pbow_gridsearch7 = GridSearchCV(\n",
    "    Pipeline([\n",
    "        (\"Model\", PersistentBow(KMeans(7, n_init=1, max_iter=300),\n",
    "                              sampler=RandomPDSampler(2500))),\n",
    "        (\"Estimator\", SVC(kernel=\"linear\", max_iter=1e4, gamma=\"auto\", tol=1e-4))\n",
    "    ]),\n",
    "    {\n",
    "        \"Model__cluster__n_clusters\" : [225],\n",
    "        \"Model__sampler__max_points\" : [12500],\n",
    "        \"Model__sampler__weight_function\" : [const, linear],\n",
    "        \"Estimator__C\" : [1],\n",
    "        \"Estimator__max_iter\" : [1e4, 1e5, 1e6, 1e7, 1e8]\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    cv=tt_pairs\n",
    ")\n",
    "\n",
    "pbow7_path = f\"./presplited/secondary/pbow7.dill\" \n",
    "grid = ut.load(pbow7_path)\n",
    "if not grid:\n",
    "    pbow_gridsearch7.verbose = 10\n",
    "    pbow_gridsearch7.fit(X, y)\n",
    "    ut.save(pbow_gridsearch7, pbow7_path)\n",
    "else:\n",
    "    pbow_gridsearch7 = grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PersistentBow(KMeans(225, n_init=1, max_iter=300),\n",
    "                              sampler=RandomPDSampler(12500, const))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MinMaxScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-c3d16a1ac0dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MinMaxScaler' is not defined"
     ]
    }
   ],
   "source": [
    "b = MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bows = model.fit_transform(X[tt_pairs[1][0]])\n",
    "test_bows = model.transform(X[tt_pairs[1][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_svm = libsvm.svmutil.svm_train(y[tt_pairs[1][0]], train_bows, f\"-s 0 -t 0 -c 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 84.0997% (2396/2849) (classification)\n"
     ]
    }
   ],
   "source": [
    "libsvm.svmutil.svm_predict(y[tt_pairs[1][1]], test_bows, c_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC(kernel=\"linear\", max_iter=1e6, gamma=\"auto\", tol=1e-4, break_ties=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=True, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "    max_iter=1000000.0, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(train_bows, y[tt_pairs[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8543348543348543"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.score(test_bows, y[tt_pairs[0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
